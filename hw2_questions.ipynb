{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE252C: Homework 2\n",
    "## Computing Resources\n",
    "Please read the README file of this repository for the instructions\n",
    "## Instructions\n",
    "1. Attempt all questions.\n",
    "2. Please comment all your code adequately.\n",
    "3. Include all relevant information such as text answers, output images in notebook.\n",
    "4. **Academic integrity:** The homework must be completed individually.\n",
    "\n",
    "5. **Submission instructions:**  \n",
    " (a) Submit the notebook and its PDF version on Gradescope.  \n",
    " (b) Rename your submission files as Lastname_Firstname.ipynb and Lastname_Firstname.pdf.  \n",
    " (c) Correctly select pages for each answer on Gradescope to allow proper grading.\n",
    "\n",
    "6. **Due date:** Assignments are due Thu, May 21, at 4pm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Using SphereFace [3] for Face Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the first section, we will test a pretrained model of SphereFace on LFW [4] dataset. The LFW dataset is on `/datasets/cse152-252-sp20-public/hw2_data/lfw`. The dataset contains 6000 pairs of human face images with ground truth labels for whether they are from the same identity.\n",
    "2. The PyTorch code of SphereFace is located in `./sphereFace`, which is modified based on the open source code from `https://github.com/clcarwin/sphereface_pytorch`. \n",
    "3. Run the following commands and report the accuracy of SphereFace on LFW verification. **(5 points)**\n",
    "```\n",
    "cd sphereFace\n",
    "tar -zxf model.tar.gz\n",
    "python lfw_eval.py --model ./model/sphere20a_20171020.pth --net faceNet --lfw /datasets/cse152-252-sp20-public/hw2_data/lfw/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Explain briefly how the following steps are performed when evaluating on LFW dataset: \n",
    "    1. Given the features extracted from the network, what is the metric used to measure the distance between two faces? (`lfw_eval.py`: Line 135) **(5 points)**\n",
    "    2. How is the threshold set to determine whether two faces are from the same identity? How is the accuracy computed? (`lfw_eval.py`: Line 141 to 148) **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. An important step before face recognition is face alignment, in which we warp and crop the image based on the location of facial landmarks.\n",
    "    1. Briefly describe how we warp and crop the image. (`lfw_eval.py`: Line 11-26) **(5 points)**\n",
    "    2. Instead of doing face alignment, crop an image patch of height 112 pixels and width 96 pixels at the center of the image. Report the accuracy.  **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Using MTCNN [5] for Detecting Face Landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Instead of using provided facial landmarks, we will now use MTCNN [5] for detecting them. The code is located at `./mtcnn`. Run the following commands to generate the facial landmarks. Include two example outputs in your report. **(5 points)**\n",
    "```\n",
    "cd MTCNN \n",
    "python lfw_landmark.py --lfw /datasets/cse152-252-sp20-public/hw2_data/lfw/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Go to the `sphereFace` directory, run the following commands by setting flag `alignmentMode` to be 2. Report the error using the predicted facial landmarks. **(5 points)**\n",
    "```\n",
    "cd sphereFace\n",
    "python lfw_eval.py --model ./model/sphere20a_20171020.pth --net faceNet --lfw /datasets/cse152-252-sp20-public/hw2_data/lfw/ --alignmentMode 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Next, answer the following questions:\n",
    "    1. Is the result better than using the landmarks provided in the previous question? If not, how can you improve performance? **(5 points)**\n",
    "    2.  What are the steps adopted by the method to achieve real-time speed? **(5 points)**\n",
    "    3. Briefly describe how non-maximal suppression (NMS) is implemented in this method. (`src/box_utils.py`: Line 5-68) **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Training SphereFace [3] on CASIA Dataset [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now train a network on the CASIA dataset [6] and test on the LFW dataset [4]. In this section, the skeleton code for training is given. CASIA-Webface dataset can be found at `/datasets/cse152-252-sp20-public/hw2_data/CASIA-WebFace`.  \n",
    "1. Go to directory of  `sphereFace` and open `faceNet.py`. Under class `CustomLinear` implement function $\\psi$ as shown in Eq. (7) and Appendix G in [3], which is as follows. \n",
    "\\begin{equation*}\n",
    "\\psi(\\theta ) = (-1)^{k}\\cos(m\\theta )-2k, \\quad \\theta \\in [\\frac{k\\pi}{m}, \\frac{(k+1)\\pi}{m}] \\nonumber\n",
    "\\end{equation*}\n",
    "Under class `CustomLoss`, implement the loss function. **(15 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Copy your implementation of CustomLinear and CustomLoss here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train the network using $\\mathtt{casia\\_train.py}$. You may try various hyperparameters like $m$, learning rate, batch size, training iterations and so on. Stop when you think the network behaves strangely (drop in accuracy, or loss stops decreasing). You may refer (and cite) any open source code (for example, \\url{https://github.com/clcarwin/sphereface_pytorch}). Include the following in your report:\n",
    "    1. Curves for training loss and accuracy on CASIA, which have been saved in the `checkpoint` directory (you may smooth the curves to make them look better). **(10 points)**\n",
    "    2. Accuracy on the LFW dataset, evaluated using `lfw_eval.py`. You are expected to achieve accuracy higher than 90% on the LFW dataset. **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer Here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The architecture above is a 20-layer residual network as described in Table 2 of [3], but without batch normalization. Now add batch normalization after every convolutional and fully connected layer. Train the new network on CASIA dataset and test on LFW dataset. Following is a demonstration of a residual block with 128 filters and kernel size $3\\times3$:\n",
    "\\begin{align}\n",
    "y &=& \\mathtt{CONV}_{3\\times3, 128}(x) \\\\\n",
    "y &=& \\mathtt{BatchNorm}(y) \\\\\n",
    "y &=& \\mathtt{PReLU}(y) \\\\\n",
    "y &=& \\mathtt{CONV}_{3\\times3, 128}(y) \\\\\n",
    "y &=& \\mathtt{BatchNorm}(y) \\\\\n",
    "y &=& \\mathtt{PReLU}(y) \\\\\n",
    "\\mathtt{OUT} &=& x + y\n",
    "\\end{align}\n",
    "    1. Draw the training curves for accuracy and loss on CASIA and compare to the curve without batch normalization. **(10 points)**\n",
    "    2. Report accuracy on the LFW dataset, evaluated using `lfw_eval.py`. **(10 points)**\n",
    "    3. Do you achieve better performance on LFW? If yes, explain how batch normalization helps. If not, try to explain why the results are worse.  **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Randomly choose 10 identities from the CASIA dataset, forward pass all their images through the network and visualize the normalized features using tSNE \\cite{maaten2008visualizing}. You can use code from \\url{https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html} for visualization. Try a few random samples and include the figure that you consider most illustrative of the method. Retain the identities chosen here since they will be used again for the next question. **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Training CosFace [7] on CASIA Dataset [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are required to implement CosFace based on the code you use in the previous section, train it on CASIA dataset and test on LFW dataset. \n",
    "1. Go to directory `./cosFace` and open `faceNet.py`. Again implement the function $\\psi$ and the loss function of CosFace under `CustomLinear` and `CustomLoss` in `faceNet.py`. You may check (and duly cite) any open source implementation for hints on improving the performance. **(15 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Copy your implementation of CustomLinear and CustomLoss here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Copy the 20-layer residual network with batch normalization you have written in the previous section to `faceNet.py`. Use that network structure for training and testing. Train the network on CASIA dataset. Again you are free to change any hyper parameters but report the hyper parameters that you think might influence the performance. Again draw the curve of loss and accuracy of training. Report the accuracy on LFW dataset. **(20 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If you achieve better performance compared to SphereFace, well done! Can you provide a reason? If you do not outperform SphereFace, can you provide a cause? **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plot the tSNE visualization of the CosFace embedding for the same identities from the CASIA dataset used to visualize the SphereFace embedding in the previous question. **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Human Pose Estimation with Convolutional Pose Machines (CPM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you will be given code adapted from [PyTorch implementation of one of the CPM model [1]](https://github.com/Hzzone/pytorch-openpose), which is an follow-up work of the original Convolution Pose Machine [2], and shares the Part Confidence Maps estimation module with [2]. In this question you will be given a trained CPM model and gain insights about the model design and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ``cd pytorch-openpose``\n",
    "- Set up the environment following the instructions at ``pytorch-openpose/README.md`` (Getting Started - Install Requriements)\n",
    "- Download the ``body_pose_model.pth`` from [Dropbox](https://www.dropbox.com/sh/7xbup2qsn7vvjxo/AABWFksdlgOMXR_r5v3RwKRYa?dl=0) and place under ``pytorch-openpose/model``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Run the inference code on the given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from src import model\n",
    "from src import util\n",
    "from src.body import Body\n",
    "from src.hand import Hand\n",
    "\n",
    "body_estimation = Body('model/body_pose_model.pth')\n",
    "\n",
    "test_image = 'images/demo.jpg'\n",
    "oriImg = cv2.imread(test_image)  # B,G,R order\n",
    "candidate, subset, heatmap_list, heatmap_list_converted_list = body_estimation(oriImg)\n",
    "heatmap_0 = heatmap_list[0] \n",
    "canvas = copy.deepcopy(oriImg)\n",
    "canvas = util.draw_bodypose(canvas, candidate, subset)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(canvas[:, :, \n",
    "                  [2, 1, 0]])\n",
    "# plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Visualize the output keypoint detection result. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Paste the output figure here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) What is the ratio of size $\\lambda=H/H'$ between the input image **im** (Line 57 of ``body.py``) \n",
    "of shape [1, 3, H, W] and the output heatmap **heatmap_0** of shape [1, D, H', W']? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer the question here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) What module in the model is reponsible for this scaling? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer the question here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ``heatmap_list_converted_list[0]`` is a list of heatmaps from all 6 layers: [out1_2, out2_2, out3_2, out4_2, out5_2, out6_2], where the output of each layer is of shape [H, W, D]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet gives the function to visualize the $d_{th}$ feature map from the layer **layer_idx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layer_idx = -1\n",
    "\n",
    "for d in range(heatmap_list_converted_list[0][layer_idx].shape[2]):\n",
    "    layer_idx = 0\n",
    "    heatmap = heatmap_list_converted_list[0][layer_idx][:, :, d]\n",
    "    util.overlay_heatmap(canvas, heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) What is D? And given D can you tell what is the number of keypoints that the model is trying to estimate? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer [1] here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Visualize and compare the heatmap from **layer_idx=0** and **layer_idx=5** for the keypoint **d=2**. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Paste and compare the visualizations here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Visualize and compare the heatmap from **layer_idx=0** and **layer_idx=5** for the keypoint **d=3**. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Paste and compare the visualizations here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) State and justify the shared difference in these comparisons? You may get some hint from the model design. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``State and justify the difference here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Set **multi_scale=False** in Line 38 of ``body.py`` and re-run the code from question 5(1). \n",
    "\n",
    "(a) Visualize the output below. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Paste figure by re-running (a) with multi_scale=False here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Do the same comparison with additional images of ``demo_coco.jpg``, ``demo_coco2.jpg``, ``demo_beach.jpg``, ``demo_ucsd.jpg``, ``demo_ucsd2.jpg``.  **(5 points)**\n",
    "\n",
    "**NOTE: you might need to restart the kernel each time you change the multi_scale flag. This may be required to re-initialize the model. [Kernel]-->[Restart]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``For each of the additional images, paste two figures with multi_scale=True and multi_scale=False here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) What shared difference can you spot in most of the comparisons? Justify the difference. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Report the difference and justify here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "[1] Cao, Zhe. et al. \"Realtime multi-person 2d pose estimation using part affinity fields.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.\n",
    "\n",
    "[2] Wei, Shih-En. et al. \"Convolutional pose machines.\" Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2016.\n",
    "\n",
    "[3] Liu, Weiyang. et al. \"SphereFace: Deep Hypersphere Embedding for Face Recognition.\" arXiv:1704.08063.\n",
    "\n",
    "[4] Huang, Gary. et al. \"Labeled faces in the wild:  Adatabase for studying face recognition in unconstrained environments.\" Technical Report 07-49, Universityof Massachusetts, Amherst, October 2007.\n",
    "\n",
    "[5] Zhang, Kaipeng. et al. \"Joint face detection and alignment usingmultitask cascaded convolutional networks.\" IEEE Signal Processing Letters, 23(10):1499–1503, 2016.\n",
    "\n",
    "[6] Yi, Dong. et al.  Learning face representation from scratch. arXiv:1411.7923.\n",
    "\n",
    "[7] Wang, Hao. et al.Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pages 5265–5274, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
